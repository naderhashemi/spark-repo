# Databricks notebook source

#==========================================================================================#
#title           :fpspy.py
#description     :This script/module will installed in python standard libraries location \
#        classes and methods to pull parametes from DB2 and also save
#       	  results to a hive table
#author          :lakshmb
#dateCreated     :2020-05-10
#dateUpdated     :2022-02-07 - environment awarness connections
#version         :POC
#                 Modified by  Srini Madabathula to make it work with environment awarness objects.
#usage           :pyspark fps2py.py OR import fps2py
#notes           :passwords encrypted using base64 library
#===========================================================================================#
#
import sys
import warnings
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline,PipelineModel
from pyspark.dbutils import DBUtils
import logging
from pyspark.conf import SparkConf

HDFS_LOC='/modelerdata/modelresults/'

# create logger
logger = logging.getLogger('FPSPY')
logger.setLevel(logging.INFO)



# Create a Spark Session
def createSparkSession():

       spark = SparkSession \
               .builder \
               .appName('getParams') \
               .enableHiveSupport() \
               .getOrCreate()
       spark.sparkContext.setLogLevel('WARN')
       return spark
       


# class to get model params
class getModel(object):
        
       def __init__(self,ModelName,Release,Env):
           spark = createSparkSession()
           self.ModelBusnsID = ModelName
           self.Release = Release
           self.Env = str(spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost())
           #self.checkModel()
           getUserid()
           getCluster()
            
       def checkModel(self):
           df=getFPSModel(self)
           if len(df.head(1)) == 0:
               warnings.warn("Model donot exist in DB")



# class to store db2 cpnfig params
def getAttr():   

           spark = createSparkSession()
           return [spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost().getJdbcURL(), spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost().getJdbcProperties().get("user"), spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost().getJdbcProperties().get("password"), spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost().getJdbcProperties().get("driver") ];       
  
  

def getEnvironment():

    spark = createSparkSession()
    return spark._jvm.com.peraton.insight.util.Util.getCurrentDatabricksEnvByServerName()
  

def getCluster():

    spark = createSparkSession()
    return spark._jvm.com.peraton.insight.util.Util.getCurrentDatabricksClusterName()

def getUserid():

    spark = createSparkSession()
    userdf = spark.sql("select current_user() as user")
    return userdf.collect()[0]

def getSnowflakeParams(env):

    spark = createSparkSession()
    return {
                   "sfURL": spark._jvm.com.peraton.insight.util.Util.getSnowflakeSparkConnPropertiesByEnv().get("sfURL"),
                   "sfUser": spark._jvm.com.peraton.insight.util.Util.getSnowflakeSparkConnPropertiesByEnv().get("sfUser"),
                   "sfPassword": spark._jvm.com.peraton.insight.util.Util.getSnowflakeSparkConnPropertiesByEnv().get("sfPassword"),
                   "sfDatabase": spark._jvm.com.peraton.insight.util.Util.getSnowflakeSparkConnPropertiesByEnv().get("sfDatabase"),
                   "sfRole": spark._jvm.com.peraton.insight.util.Util.getSnowflakeSparkConnPropertiesByEnv().get("sfRole"),
                   "sfSchema": "FPS_MODELS",
                   "sfWarehouse": spark._jvm.com.peraton.insight.util.Util.getSnowflakeSparkConnPropertiesByEnv().get("sfWarehouse"),
                   "parallelism": "64"
                };


   
def getSnowflakeSource():

  return "net.snowflake.spark.snowflake";



# method that extracts model params as DataFrame from MSTR_MODEL_CONFIG
def getParamsAsSDF(model):

       spark = createSparkSession()
       attr=getAttr()
       url=attr[0]
       usr=attr[1]
       pwd=attr[2]
       driver=attr[3]

       # connect to DB2 and get the entire table to DataFrame
       try:
           df = spark.read.format('jdbc') \
                .option('url', url) \
                .option('dbtable', spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost().getConfigTableName()) \
                .option('driver', driver) \
                .option('user', usr) \
                .option('password', pwd) \
                .load()
       except:
           raise Exception("Error: Unable to connect to DB2")


       df=df.filter(df.MODEL_BUSNS_ID == model.ModelBusnsID)
       df=df.select('CONFIG_FLD','CONFIG_FLD_VAL')
       return df



# method that extracts model params as Dict from MSTR_MODEL_CONFIG
def getParamsAsDict(model):

       # Get params as dataframe
       df=getParamsAsSDF(model)

       # Convert DF to Dict
       newrdd = df.rdd.map(lambda x : (x[0],x[1]))
       dict = newrdd.collectAsMap()

       return dict

  

# method that extracts model params from FPS_MODEL
def getFPSModel(model):

       spark = createSparkSession()
       attr=getAttr()
       url=attr[0]
       usr=attr[1]
       pwd=attr[2]
       driver=attr[3]

       try:
           df = spark.read.format('jdbc') \
                       .option('url', url) \
                       .option('dbtable', spark._jvm.com.peraton.insight.util.FPSEnvironment.getForThisHost().getResultsTableName("FPS_MODEL")) \
                       .option('driver', driver) \
                       .option('user', usr) \
                       .option('password', pwd) \
                       .load()
       except:
           logger.debug("Error: Unable to connect to DB2")
           sys.exit(1)

       df=df.filter(df.MODEL_BUSNS_ID == model.ModelBusnsID)
       return df

      

# method to save DF to Hive
def send(model,df,tblsfx):

       sfxList=['npisuspect','claimsatrisk','claimsatriskptd','reportcard','reportcardsna','supplementalrc']
       tblsfx=tblsfx.lower()
       if tblsfx not in sfxList:
           raise Exception('Error: Table name suffix {} not in the list'.format(tblsfx))
            
       spark=createSparkSession()
       #SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
       SNOWFLAKE_SOURCE_NAME = getSnowflakeSource()
       sfOptions = getSnowflakeParams(model.Env)
       tblName="fps_models.{0}_{1}".format(model.ModelBusnsID,tblsfx)
       df.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", tblName).mode("overwrite").save()
       logger.info("Completed sending data to table {}".format(tblName))



# method to persist spark dataframe to specified table in hive
def persistDF(df,tblName, env):

       spark=createSparkSession()
       SNOWFLAKE_SOURCE_NAME = getSnowflakeSource()
       sfOptions = getSnowflakeParams(env)
       df.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", tblName.strip().lower()).mode("overwrite").save()
       logger.info("Completed persisting data to table {}".format(tblName))



# method to persist spark dataframe to specified table in hive
def persistDF2(df, dbSchema, tblName, env):

       spark=createSparkSession()
       SNOWFLAKE_SOURCE_NAME = getSnowflakeSource()
       sfOptions = getSnowflakeParams(env)
       df.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("sfSchema", dbSchema.strip().lower()).option("dbtable", tblName.strip().lower()).mode("overwrite").save()
       logger.info("Completed persisting data to table {}".format(tblName))



def save(model,pipeline):

       pipeType=pipeline.uid.split('_')[0]
       if pipeType == 'PipelineModel':
           pipedirname='trainedpipeline'
       elif pipeType == 'Pipeline':
           pipedirname='pipeline'

       modelname=model.ModelBusnsID
       out_hdfs_path='{0}/{1}/{2}'.format(HDFS_LOC,modelname,pipedirname)
       pipeline.write().overwrite().save(out_hdfs_path)



# load a saved pipeline/pipelinemodel
def load(model,pipelineType):

       modelname=model.ModelBusnsID
       pipeType=pipelineType.lower()

       if pipeType == 'pipelinemodel':
           pipeLoc='{0}/{1}/trainedpipeline'.format(HDFS_LOC,modelname)
           try:
               loadedPipeline=PipelineModel.load(pipeLoc)
           except:
               logger.debug('ERROR: No PipelineModel exist for loading')
               return
       elif pipeType == 'pipeline':
           pipeLoc='{0}/{1}/pipeline'.format(HDFS_LOC,modelname)
           try:
               loadedPipeline=Pipeline.load(pipeLoc)
           except:
               logger.debug('ERROR: No Pipeline exist for loading')
               return
       else:
          raise Exception('Error: Invalid Pipeline Type')

       return loadedPipeline
    
#' query snowflake table to a dataframe
def sqlHive(model,query):
  return queryWH(model,query)
  
def queryWH(model,query):
  spark=createSparkSession()
  SNOWFLAKE_SOURCE_NAME = getSnowflakeSource()
  sfOptions = getSnowflakeParams(model.Env)
  df=spark.read.format("snowflake").options(**sfOptions).option("query", query).load()
  logger.info("Completed queryWH data to table {}")
  return df 
  
  
  
def referenceDate (model,  dt=None):

 
  spark = createSparkSession()
  dbutils = DBUtils(spark)
  

  try:
     refdate = dbutils.widgets.get("enddate")
     #logger.debug('Obtained refdate from DBUTILS')
  except:
     refdate =  None
     
  if refdate is None:
    refdate = dt
       
  return refdate
  
